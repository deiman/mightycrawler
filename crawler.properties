# MANDATORY: start at the given url. host names must end with /, eg. http://127.0.0.1/
startURL=http://127.0.0.1/

# restrict crawling and downloading to urls matching these regex patterns. defaults to everything under startURL.
#includeURL=

# among the included urls, exclude any urls matching these regex patterns. defaults to none (don't exclude any).
excludeURL=

# restrict parsing and downloading content types matching these regex patterns. defaults to none - if nothing is specified.
includeLinkExtraction=text/html

# among the included content types, exclude any matching these regex patterns. defaults to none (don't exclude any).
excludeLinkExtraction=

# restrict downloading content types matching these regex patterns. defaults to none - if nothing is specified.
# javascript = text/javascript, application/x-javascript, application/javascript etc, see http://www.rfc-editor.org/rfc/rfc4329.txt
includeContent=text/html

# among the included content types, exclude any matching these regex patterns. defaults to none (don't exclude any).
excludeContent=

# regard URLs having content types matching this regex pattern as an URL to a binary document file.
# binary files are not downloaded and not parsed - instead their URLs are put in a separate file for possible later processing.
# note: only URLs found in content that matches the link extraction filter will be considered.
#		also, only those links to binaries that are found inside href attributes will be found.
# if nothing is specified, no URLs are added.
includeBinaryFile=application/pdf|image/svg+xml

# among the included content types, exclude any matching this regex pattern. defaults to none (don't exclude any).
excludeBinaryFile=

# URLs to binary files will be appended to this file. if filename is blank no file is generated.
binariesFile=binaries.txt

# the user agent the crawler reports as.
userAgent=

# when storing pages to disk, use this file name for the URLs that end with /
defaultPage=index.html

# when storing pages to disk, use this encoding unless specified in the content-type response header.
# must be a valid Java encoding, see http://java.sun.com/javase/6/docs/api/java/nio/charset/Charset.html
defaultEncoding=UTF-8

# number of threads for downloading.
downloadThreads=1

# number of threads for parsing.
parseThreads=1

# number of threads for saving to disk.
saveThreads=1

# download up to n pages.
maxPages=20

# let each download thread wait n seconds before it starts.
downloadDelay=10

# stop crawling if no new urls are found within n seconds.
crawlerTimeout=30

# MANDATORY: where to put the downloaded web pages
outputDirectory=./webpages/

# MANDATORY: where to put download statistics
reportDirectory=./reports/

# SQL statements for generating reports after completion
reportSQL=select http_code, url from downloads where http_code <> 200@httperrors.txt \
	|select url from downloads where LENGTH(url) > 150@longurls.txt \
	|select links.url_to, links.url_from from links, downloads where links.url_to = downloads.url and downloads.downloaded=true@parentpages.txt \
	|select downloaded_at, url from downloads where downloaded=true@timestamps.txt \
	|select COUNT(*) + ' pages downloaded.' from downloads where downloaded=true@pagecount.txt \
	|select 'Crawling took ' + 0.001 * DATEDIFF('ms', MIN(downloaded_at), MAX(downloaded_at)) + ' seconds.' from downloads where downloaded=true@crawlertime.txt \
	|select 'Average response time: ' + 0.001 * SUM(response_time) / COUNT(*) + ' seconds.' from downloads where downloaded=true@avgresponsetime.txt \
	|select response_time, url from downloads where downloaded=true@responsetimes.txt \
	|select response_time, url from downloads where downloaded=true order by response_time desc@responsetimes_sorted.txt \
	|select url from downloads where downloaded=true@urls.txt
	

# data model for the download statistics:
#
# downloads
# url VARCHAR(4095) | http_code INTEGER default 0 | response_time INTEGER default 0 | downloaded_at DATETIME default NOW | downladed BOOLEAN
#
# links
# url_from VARCHAR(4095) | url_to VARCHAR(4095)
#